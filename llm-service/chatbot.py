import os
import json
import warnings
from dotenv import load_dotenv
from pathlib import Path
from collections import defaultdict
from typing import TypedDict, Optional, List, Dict, Any
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain, RetrievalQA
from langgraph.graph import StateGraph, END
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_core.messages import BaseMessage
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama



# Load API key and suppress warnings
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
warnings.filterwarnings("ignore")

# LLM ì„¤ì •
# llm_config = {
#     "api_key": OPENAI_API_KEY,
#     "base_url": "https://api.groq.com/openai/v1",
#     "model": "meta-llama/llama-4-scout-17b-16e-instruct",
#     "temperature": 0.7
# }
# llm_split = ChatGroq(**llm_config)
# llm_feedback = ChatGroq(**llm_config)

llm_split = Ollama(model="qwen3:1.7b")
llm_feedback = Ollama(model="qwen3:1.7b")

# --- ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict):
    company_name: Optional[str]
    news_articles: Optional[List[Dict]]
    news_summary: Optional[str]
    chat_history: List[BaseMessage]
    uploaded_file_content: Optional[str]
    enhanced_resume: Optional[str]
    user_question: Optional[str]
    next_node: Optional[str]
    file_path: Optional[str]
    section_map: Optional[Dict]
    classified: Optional[List]
    pages: Optional[List]
    vectorstore: Optional[Any]
    company_analysis: Optional[str]
    feedback: Optional[str]
    answer: Optional[str]

# --- í”„ë¡¬í”„íŠ¸ ì„¤ì • ---
route_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì…ë ¥ì…ë‹ˆë‹¤:

"{question}"

ì´ ì…ë ¥ì€ ì–´ë–¤ ì‘ì—…ì„ ìš”ì²­í•˜ê³  ìˆë‚˜ìš”?

- ì¼ë°˜ ì •ë³´ ì§ˆë¬¸ì´ë©´ "qa"
- ì²¨ë¶€ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”¼ë“œë°± ìš”ì²­ì´ë©´ "feedback"
- ë‰´ìŠ¤ ìš”ì•½ ìš”ì²­ì´ë©´ "summarize"

ì´ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”.
""")
route_chain = LLMChain(llm=llm_split, prompt=route_prompt)

# # --- ë¶„ê¸° ë…¸ë“œ ---
# def route_by_input_type(state: GraphState) -> str:
#     print("ğŸ” LLMìœ¼ë¡œ next_node íŒë‹¨ ì¤‘...")
#     result = route_chain.run(question=state["user_question"]).strip().lower()

#     if result == "qa":
#         return "answer_question"
#     elif result == "feedback":
#         return "LoadPDF"
#     elif result == "summarize":
#         return "summarize_news"
#     else:
#         raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” next_node ì‘ë‹µ: {result}")

# # --- ë…¸ë“œ ì •ì˜ ---
# def load_resume_pdf(state: GraphState) -> GraphState:
#     loader = PyPDFLoader(state["file_path"])
#     return {**state, "pages": loader.load()}

# def classify_by_page(state: GraphState) -> GraphState:
#     results = []
#     for idx, page in enumerate(state["pages"]):
#         res = chain_split.run(page_text=page.page_content.strip())
#         results.append({
#             "page": idx + 1,
#             "category": res,
#             "content": page.page_content.strip()
#         })
#     return {**state, "classified": results}

# def make_section_map(state: GraphState) -> GraphState:
#     section_map = defaultdict(list)
#     for item in state["classified"]:
#         section = item['category'].split(":")[0].strip()
#         section_map[section].append(item['content'])
#     return {**state, "section_map": section_map}

# def vector_indexing(state: GraphState) -> GraphState:
#     file_path = state["file_path"]
#     filename = os.path.splitext(os.path.basename(file_path))[0]
#     save_dir = f"./vectorstore/{filename}"
#     Path(save_dir).mkdir(parents=True, exist_ok=True)

#     embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
#     if os.path.exists(os.path.join(save_dir, "index.faiss")):
#         print(f"[ë¡œë“œë¨] {save_dir}")
#         vectorstore = FAISS.load_local(save_dir, embeddings)
#     else:
#         texts, metadatas = [], []
#         for section, contents in state["section_map"].items():
#             for content in contents:
#                 texts.append(content)
#                 metadatas.append({"section": section})
#         vectorstore = FAISS.from_texts(texts, embeddings, metadatas)
#         vectorstore.save_local(save_dir)
#         print(f"[ì €ì¥ ì™„ë£Œ] {save_dir}")

#     return {**state, "vectorstore": vectorstore}

# def load_company_analysis(state: GraphState) -> GraphState:
#     return state

# def match_and_feedback(state: GraphState) -> GraphState:
#     retriever = state["vectorstore"].as_retriever()
#     qa_chain = RetrievalQA.from_chain_type(llm=llm_feedback, retriever=retriever)
#     prompt = f"""
#     ë‹¤ìŒì€ í•œ ê¸°ì—…ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤:

#     "{state['company_analysis']}"

#     ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ í•­ëª©ë³„ë¡œ, ê°•ì¡°í•  ì  / ë¶€ì¡±í•œ ì  / ë³´ì™„ì ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
#     """
#     return {**state, "feedback": qa_chain.run(prompt)}

# def answer_question(state: GraphState) -> GraphState:
#     retriever = state["vectorstore"].as_retriever()
#     qa_chain = RetrievalQA.from_chain_type(llm=llm_feedback, retriever=retriever)
#     result = qa_chain.run(state["user_question"])
#     return {**state, "answer": result}

# def summarize_news(state: GraphState) -> GraphState:
#     return {**state, "news_summary": f"[ìš”ì•½ ê²°ê³¼] ë‰´ìŠ¤ ìš”ì•½ ìš”ì²­ë¨: {state['user_question']}"}

# --- ì´ë ¥ì„œ í˜ì´ì§€ ë¶„ë¥˜ìš© LLM ì„¤ì • ---
split_prompt = ChatPromptTemplate.from_template("""
ì•„ë˜ëŠ” ì´ë ¥ì„œ ë˜ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ì˜ í•œ í˜ì´ì§€ì…ë‹ˆë‹¤:

"{page_text}"

ì´ í˜ì´ì§€ëŠ” ì–´ë–¤ í•­ëª©ì— í•´ë‹¹í•˜ë‚˜ìš”?
(ì˜ˆ: ê²½ë ¥, í•™ë ¥, í”„ë¡œì íŠ¸, ê¸°ìˆ , ìê¸°ì†Œê°œ ë“±)

í•œ ë‹¨ì–´ë¡œ í•­ëª©ì„ ë¶„ë¥˜í•˜ê³ , ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
""")
chain_split = LLMChain(llm=llm_split, prompt=split_prompt)

# # --- ì‹¤í–‰ í•¨ìˆ˜ ---
# def run_langgraph_flow(
#     user_question: str,
#     resume_path: Optional[str] = None,
#     news_full_path: Optional[str] = None,
#     news_summary_path: Optional[str] = None,
#     chat_history: Optional[List[BaseMessage]] = None,
# ) -> Dict:
#     state: GraphState = {
#         "user_question": user_question,
#         "chat_history": chat_history or []
#     }

#     if resume_path and Path(resume_path).exists():
#         state["file_path"] = resume_path
#         state["uploaded_file_content"] = "uploaded"

#     if news_summary_path and Path(news_summary_path).exists():
#         with open(news_summary_path, "r", encoding="utf-8") as f:
#             news_json = json.load(f)
#             state["company_analysis"] = news_json.get("summary") or news_json.get("content")

#     if news_full_path and Path(news_full_path).exists():
#         with open(news_full_path, "r", encoding="utf-8") as f:
#             full_news_text = f.read()
#         embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
#         vectorstore = FAISS.from_texts([full_news_text], embeddings)
#         state["vectorstore"] = vectorstore

#     graph = StateGraph(GraphState)

#     # ë…¸ë“œ ë“±ë¡
#     graph.add_node("router", route_by_input_type)
#     graph.add_node("answer_question", answer_question)
#     graph.add_node("summarize_news", summarize_news)

#     # ì´ë ¥ì„œ ë¶„ì„ íë¦„
#     graph.add_node("LoadPDF", load_resume_pdf)
#     graph.add_node("ClassifyPages", classify_by_page)
#     graph.add_node("ToSectionMap", make_section_map)
#     graph.add_node("VectorIndexing", vector_indexing)
#     graph.add_node("LoadCompanyInfo", load_company_analysis)
#     graph.add_node("Feedback", match_and_feedback)

#     # ë¼ìš°í„° ì„¤ì •
#     graph.set_entry_point("router")
#     graph.add_conditional_edges("router", route_by_input_type, {
#         "answer_question": END,
#         "summarize_news": END,
#         "LoadPDF": "ClassifyPages"
#     })

#     graph.add_edge("ClassifyPages", "ToSectionMap")
#     graph.add_edge("ToSectionMap", "VectorIndexing")
#     graph.add_edge("VectorIndexing", "LoadCompanyInfo")
#     graph.add_edge("LoadCompanyInfo", "Feedback")
#     graph.add_edge("Feedback", END)

#     # ì‹¤í–‰
#     compiled = graph.compile()
#     result = compiled.invoke(state)
#     return result

# --- ë¶„ê¸° ë…¸ë“œ ---
def route_by_input_type(state: GraphState) -> str:
    print("ğŸ”¹ [route_by_input_type] í•¨ìˆ˜ ì§„ì…")
    print("ğŸ” LLMìœ¼ë¡œ next_node íŒë‹¨ ì¤‘...")
    result = route_chain.run(question=state["user_question"]).strip().lower()
    print(f"ğŸª next_node ê²°ì •: {result}")

    if result == "qa":
        return "answer_question"
    elif result == "feedback":
        return "LoadPDF"
    elif result == "summarize":
        return "summarize_news"
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” next_node ì‘ë‹µ: {result}")

# --- ë…¸ë“œ ì •ì˜ ---
def load_resume_pdf(state: GraphState) -> GraphState:
    print("ğŸ”¹ [load_resume_pdf] ì´ë ¥ì„œ PDF ë¡œë“œ ì¤‘...")
    loader = PyPDFLoader(state["file_path"])
    pages = loader.load()
    print(f"âœ… {len(pages)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
    return {**state, "pages": pages}

def classify_by_page(state: GraphState) -> GraphState:
    print("ğŸ”¹ [classify_by_page] í˜ì´ì§€ë³„ ë¶„ë¥˜ ì¤‘...")
    results = []
    for idx, page in enumerate(state["pages"]):
        print(f"ğŸ—‚ï¸ í˜ì´ì§€ {idx + 1} ë¶„ë¥˜ ì¤‘...")
        res = chain_split.run(page_text=page.page_content.strip())
        results.append({
            "page": idx + 1,
            "category": res,
            "content": page.page_content.strip()
        })
        print(f"âœ… í˜ì´ì§€ {idx + 1} ë¶„ë¥˜ ê²°ê³¼: {res}")
    return {**state, "classified": results}

def make_section_map(state: GraphState) -> GraphState:
    print("ğŸ”¹ [make_section_map] ì„¹ì…˜ë³„ë¡œ ë‚´ìš© ë§¤í•‘ ì¤‘...")
    section_map = defaultdict(list)
    for item in state["classified"]:
        section = item['category'].split(":")[0].strip()
        section_map[section].append(item['content'])
    print(f"âœ… ì„¹ì…˜ ë§µí•‘ ì™„ë£Œ: {list(section_map.keys())}")
    return {**state, "section_map": section_map}

def vector_indexing(state: GraphState) -> GraphState:
    print("ğŸ”¹ [vector_indexing] ë²¡í„°ìŠ¤í† ì–´ ì¸ë±ì‹± ì¤‘...")
    file_path = state["file_path"]
    filename = os.path.splitext(os.path.basename(file_path))[0]
    save_dir = f"./vectorstore/{filename}"
    Path(save_dir).mkdir(parents=True, exist_ok=True)

    # embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    if os.path.exists(os.path.join(save_dir, "index.faiss")):
        print(f"ğŸ“‚ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ: {save_dir}")
        vectorstore = FAISS.load_local(save_dir, embeddings)
    else:
        texts, metadatas = [], []
        for section, contents in state["section_map"].items():
            for content in contents:
                texts.append(content)
                metadatas.append({"section": section})
        print(f"ğŸ’¾ ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥: {save_dir}")
        vectorstore = FAISS.from_texts(texts, embeddings, metadatas)
        vectorstore.save_local(save_dir)
    print("âœ… ì¸ë±ì‹± ì™„ë£Œ")
    return {**state, "vectorstore": vectorstore}

def load_company_analysis(state: GraphState) -> GraphState:
    print("ğŸ”¹ [load_company_analysis] í•¨ìˆ˜ ì§„ì…")
    return state

def match_and_feedback(state: GraphState) -> GraphState:
    print("ğŸ”¹ [match_and_feedback] í”¼ë“œë°± ìƒì„± ì¤‘...")
    retriever = state["vectorstore"].as_retriever()
    qa_chain = RetrievalQA.from_chain_type(llm=llm_feedback, retriever=retriever)
    prompt = f"""
    ë‹¤ìŒì€ í•œ ê¸°ì—…ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤:

    "{state['company_analysis']}"

    ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ í•­ëª©ë³„ë¡œ, ê°•ì¡°í•  ì  / ë¶€ì¡±í•œ ì  / ë³´ì™„ì ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
    """
    feedback = qa_chain.run(prompt)
    print("âœ… í”¼ë“œë°± ìƒì„± ì™„ë£Œ")
    return {**state, "feedback": feedback}

def answer_question(state: GraphState) -> GraphState:
    print("ğŸ”¹ [answer_question] ì§ˆë¬¸ ë‹µë³€ ì¤‘...")
    retriever = state["vectorstore"].as_retriever()
    qa_chain = RetrievalQA.from_chain_type(llm=llm_feedback, retriever=retriever)
    result = qa_chain.run(state["user_question"])
    print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
    return {**state, "answer": result}

def summarize_news(state: GraphState) -> GraphState:
    print("ğŸ”¹ [summarize_news] ë‰´ìŠ¤ ìš”ì•½ ì¤‘...")
    summary = f"[ìš”ì•½ ê²°ê³¼] ë‰´ìŠ¤ ìš”ì•½ ìš”ì²­ë¨: {state['user_question']}"
    print("âœ… ìš”ì•½ ì™„ë£Œ")
    return {**state, "news_summary": summary}

# --- ì‹¤í–‰ í•¨ìˆ˜ ---
def run_langgraph_flow(
    user_question: str,
    resume_path: Optional[str] = None,
    news_full_path: Optional[str] = None,
    news_summary_path: Optional[str] = None,
    chat_history: Optional[List[BaseMessage]] = None,
) -> Dict:
    print("ğŸš€ [run_langgraph_flow] ì‹¤í–‰ ì‹œì‘")
    state: GraphState = {
        "user_question": user_question,
        "chat_history": chat_history or []
    }

    if resume_path and Path(resume_path).exists():
        print(f"ğŸ“„ ì´ë ¥ì„œ íŒŒì¼ ê°ì§€: {resume_path}")
        state["file_path"] = resume_path
        state["uploaded_file_content"] = "uploaded"

    if news_summary_path and Path(news_summary_path).exists():
        print(f"ğŸ“° ë‰´ìŠ¤ ìš”ì•½ íŒŒì¼ ê°ì§€: {news_summary_path}")
        with open(news_summary_path, "r", encoding="utf-8") as f:
            news_json = json.load(f)
            state["company_analysis"] = news_json.get("summary") or news_json.get("content")

    if news_full_path and Path(news_full_path).exists():
        print(f"ğŸ“° ë‰´ìŠ¤ ì „ë¬¸ íŒŒì¼ ê°ì§€ ë° ì¸ë±ì‹±: {news_full_path}")
        with open(news_full_path, "r", encoding="utf-8") as f:
            full_news_text = f.read()
        # embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
        vectorstore = FAISS.from_texts([full_news_text], embeddings)
        state["vectorstore"] = vectorstore

    print("ğŸ› ï¸ ê·¸ë˜í”„ ë¹Œë“œ ì¤‘...")
    graph = StateGraph(GraphState)

    # ë…¸ë“œ ë“±ë¡
    graph.add_node("router", route_by_input_type)
    graph.add_node("answer_question", answer_question)
    graph.add_node("summarize_news", summarize_news)

    # ì´ë ¥ì„œ ë¶„ì„ íë¦„
    graph.add_node("LoadPDF", load_resume_pdf)
    graph.add_node("ClassifyPages", classify_by_page)
    graph.add_node("ToSectionMap", make_section_map)
    graph.add_node("VectorIndexing", vector_indexing)
    graph.add_node("LoadCompanyInfo", load_company_analysis)
    graph.add_node("Feedback", match_and_feedback)

    # ë¼ìš°í„° ì„¤ì •
    graph.set_entry_point("router")
    graph.add_conditional_edges("router", route_by_input_type, {
        "answer_question": END,
        "summarize_news": END,
        "LoadPDF": "ClassifyPages"
    })

    graph.add_edge("ClassifyPages", "ToSectionMap")
    graph.add_edge("ToSectionMap", "VectorIndexing")
    graph.add_edge("VectorIndexing", "LoadCompanyInfo")
    graph.add_edge("LoadCompanyInfo", "Feedback")
    graph.add_edge("Feedback", END)

    # ì‹¤í–‰
    print("ğŸ ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘")
    compiled = graph.compile()
    result = compiled.invoke(state)
    print("âœ… ê·¸ë˜í”„ ì‹¤í–‰ ì™„ë£Œ")
    return result
