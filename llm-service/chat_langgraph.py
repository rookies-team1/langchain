# poetry run python ./llm-service/chat_langgraph_2.py

import os
import sys

from langchain_openai import ChatOpenAI

# llm-service ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from typing import TypedDict, Optional, List, Dict, Any
from dotenv import load_dotenv
from collections import defaultdict
from langchain_tavily import TavilySearch
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMChain, RetrievalQA
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import OllamaEmbeddings
from langsmith import Client
from langsmith import traceable
from langsmith import traceable
import json
from langchain_chroma import Chroma
from chromadb import chromadb
import re
import chromadb

# ë¡œì»¬ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# chroma_client = chromadb.HttpClient(host="localhost", port=8001)

# ==============================================================================
# 1. ì´ˆê¸°í™” ë° ì„¤ì •
# ==============================================================================

llm = None
embeddings = None
tavily_tool = TavilySearch(k=3)

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

# LangSmith API Key ì„¤ì •
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
LANGSMITH_PROJECT = os.getenv("LANGSMITH_PROJECT", "llm-service-already")
LANGSMITH_TRACING = os.getenv("LANGSMITH_TRACING", "true").lower() == "true"
LANGSMITH_ENDPOINT = os.getenv("LANGSMITH_ENDPOINT")
client = Client(api_key=LANGSMITH_API_KEY)

def get_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
    global llm
    if llm is None:
        load_dotenv()
        # llm = ChatGoogleGenerativeAI(
        #     model="gemini-2.5-pro",
        #     temperature=0.7,
        #     max_tokens=None,
        #     timeout=None,
        #     max_retries=2,
        # )
        llm = ChatOpenAI(
            api_key=OPENAI_API_KEY,
            base_url="https://api.groq.com/openai/v1",  # Groq API ì—”ë“œí¬ì¸íŠ¸
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            temperature=0.7
        )
    return llm

def get_embeddings():
    """ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
    global embeddings
    if embeddings is None:
        load_dotenv()
        try:
            # ChromaDBì™€ ê°™ì€ ì˜êµ¬ì ì¸ ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ, ì¼ê´€ëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©ì´ ì¤‘ìš”
            embeddings = OllamaEmbeddings(
                model="bge-m3:latest", 
                base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            )
        except Exception as e:
            print(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    return embeddings


def get_chroma_client():
    """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
    global chroma_client
    if chroma_client is None:
        load_dotenv()
        # í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ChromaDBì— ì—°ê²°
        CHROMA_HOST = os.getenv("VECTOR_DB_HOST", "localhost")
        CHROMA_PORT = int(os.getenv("VECTOR_DB_PORT", "8001")) # ChromaDB ê¸°ë³¸ í¬íŠ¸ëŠ” 8000ì´ë‚˜, docker-compose ì˜ˆì‹œì—ì„œ 8001ë¡œ ì„¤ì •
        chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
    return chroma_client

# ==============================================================================
# 2. Graph State ì •ì˜
# ==============================================================================

class GraphState(TypedDict):
    # ì…ë ¥ ê°’
    session_id: int
    user_id: int
    question: str
    news_id: int  # ë‰´ìŠ¤ ì‹ë³„ì
    file_path: Optional[str]
    company: Optional[str]  # ê¸°ì—…ëª… (Tavily ê²€ìƒ‰ì— ì‚¬ìš©)
    chat_history: List[BaseMessage]
    # ê·¸ë˜í”„ ë‚´ë¶€ì—ì„œ ê´€ë¦¬ë˜ëŠ” ê°’
    input_type: str  # 'qa' or 'feedback'
    # question: str  # ì¬êµ¬ì„±ëœ ì§ˆë¬¸
    
    relevant_chunks: List[str]
    # QA ê²½ë¡œ ê´€ë ¨ ìƒíƒœ
    retriever: Optional[Any] # Retriever ê°ì²´ ì €ì¥ (ìˆ˜ì •: ìƒíƒœì— retriever ì¶”ê°€)
    
    is_grounded: bool
    tavily_snippets: Optional[List[str]]  # Tavily ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí«

    # Feedback ê²½ë¡œ ê´€ë ¨ ìƒíƒœ
    pages: Optional[List]
    user_file_summary: Optional[str]

    # ë‹µë³€ ê´€ë¦¬
    answer: str

# ==============================================================================
# 3. LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
# ==============================================================================

def clean_llm_output(text: str) -> str:
    # LLMì˜ ì¶œë ¥ë¬¼ì—ì„œ í•„ìš” ì—†ëŠ” íƒœê·¸/ë§ˆí¬ë‹¤ìš´/ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì •ë¦¬

    # ë¶ˆí•„ìš”í•œ ì—­ìŠ¬ë˜ì‹œ 2ê°œ ì´ìƒ â†’ 1ê°œë¡œ ì¹˜í™˜
    text = re.sub(r'\\\\+', r'\\', text)
    # \_ â†’ _ ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
    text = text.replace('\\_', '_')
    # <think>...</think> ë¸”ë¡ ì œê±°
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE)
    # ì¶œë ¥ ì‹œì‘/ëì— markdown blockì´ ë‚¨ëŠ” ê²½ìš° ì œê±°
    text = text.strip()
    # markdown block ì•ˆì—ë§Œ ë‚¨ì•„ìˆëŠ” ê²½ìš° ì˜ë¼ë‚´ê¸°
    # ì˜ˆ: ```markdown ... ``` êµ¬ì¡° ì œê±°
    text = re.sub(r"```(?:markdown)?\s*(.*?)```", r"\1", text, flags=re.DOTALL | re.IGNORECASE)
    # ì—°ì†ë˜ëŠ” 3ì¤„ ì´ìƒ ì¤„ë°”ê¿ˆì€ 2ì¤„ë¡œ ì¶•ì†Œ
    text = re.sub(r"\n{3,}", "\n\n", text)
     # ```json ... ``` ë“± ì½”ë“œë¸”ë¡ ì œê±°
    text = re.sub(r"```json?(.*?)```", r"\1", text, flags=re.DOTALL|re.IGNORECASE)
    # í˜¹ì€ ``` ... ``` ì „ì²´ ì œê±°
    text = re.sub(r"```.*?```", "", text, flags=re.DOTALL)
    # í•„ìš” ì—†ëŠ” ì„ ë‘/í›„ë¯¸ ê³µë°± ì œê±°
    return text.strip()


def clean_pdf_text(text: str) -> str:
    # 1) ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì¤„ì´ê¸°
    text = re.sub(r'\n+', '\n', text)
    
    # 2) ë‹¨ì–´ ì‚¬ì´ ì¤„ë°”ê¿ˆ(\n) -> ê³µë°±ìœ¼ë¡œ ëŒ€ì²´ (ë‹¨, ë¬¸ì¥ ë \nì€ ì‚´ë¦´ ìˆ˜ ìˆìŒ)
    # ì˜ˆ: 'ì„ë² ë””ë“œ\nì‹œìŠ¤í…œ' â†’ 'ì„ë² ë””ë“œ ì‹œìŠ¤í…œ'
    text = re.sub(r'(?<=\S)\n(?=\S)', ' ', text)
    
    # 3) ë‹¤ì¤‘ ê³µë°±ì„ í•œ ì¹¸ ê³µë°±ìœ¼ë¡œ ì¶•ì†Œ
    text = re.sub(r'[ \t]+', ' ', text)
    
    # 4) ë¬¸ì¥ ë¶€í˜¸ ë’¤ì—ëŠ” ì¤„ë°”ê¿ˆ ì‚´ë¦¬ê³  ë‚˜ë¨¸ì§€ëŠ” ë„ì–´ì“°ê¸°
    # (í•„ìš” ì‹œ ì»¤ìŠ¤í…€)
    
    return text.strip()
import json

@traceable(run_type="chain", name="Simple_Chain")
def retrieve_from_chroma_node(state: GraphState) -> GraphState:
    """ChromaDBì—ì„œ news_idë¥¼ í•„í„°ë§í•˜ì—¬ ê´€ë ¨ ë‰´ìŠ¤ ì²­í¬ë¥¼ ê²€ìƒ‰í•˜ì—¬ state['relevant_chunks']ì— í• ë‹¹."""

    print(f"--- 1. ChromaDB ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘ (news_id={state['news_id']}) ---")
    
    embeddings = get_embeddings()
    chroma_client = get_chroma_client()
    
    vectorstore = Chroma(
        client=chroma_client,
        collection_name="news_vector_db",
        embedding_function=embeddings,
    )
    
    news_id_filter = str(state['news_id'])
    
    retriever = vectorstore.as_retriever(
        search_kwargs={
            'filter': {'news_id': news_id_filter},
            'k': 5  # í•„ìš”ì‹œ ê°œìˆ˜ ì¡°ì •
        }
    )
    
    question = state['question']
    
    try:
        if hasattr(retriever, 'invoke'):
            documents = retriever.invoke(question)
        else:
            documents = retriever.invoke(question)

    except Exception as e:
        print(f"âŒ Chroma ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        state['relevant_chunks'] = []
        return state

    if not documents:
        print(f"âš ï¸ news_id '{news_id_filter}'ì— í•´ë‹¹í•˜ëŠ” ê´€ë ¨ ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        state['relevant_chunks'] = []
        return state

    extracted_chunks = []
    for idx, doc in enumerate(documents):
        raw_content = doc.page_content.strip()
        if not raw_content:
            print(f"âš ï¸ ë¹ˆ ì½˜í…ì¸  ë°œê²¬ (index={idx}), ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        try:
            # JSON í˜•ì‹ì´ë¼ íŒë‹¨ë˜ë©´ íŒŒì‹± ì‹œë„
            if raw_content.startswith('{') and raw_content.endswith('}'):
                parsed_json = json.loads(raw_content)
                # data.contents ê²½ë¡œì— ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ìˆì„ ê²½ìš°ë§Œ ì¶”ì¶œ
                extracted_text = parsed_json.get("data", {}).get("contents", raw_content)
            else:
                extracted_text = raw_content
        except Exception as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ì „ì²´ raw_content ì‚¬ìš© (index={idx}): {e}")
            extracted_text = raw_content

        extracted_chunks.append(extracted_text)

    state['relevant_chunks'] = extracted_chunks

    print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: '{question[:30]}...' ì— ëŒ€í•´ {len(extracted_chunks)}ê°œì˜ ê´€ë ¨ ì²­í¬ë¥¼ í• ë‹¹í–ˆìŠµë‹ˆë‹¤.")
    return state


# --- ë¼ìš°íŒ… ë…¸ë“œ ---
@traceable(run_type="chain", name="Simple_Chain")
def route_request_node(state: GraphState) -> dict:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„°"""
    print("--- 2. ìš”ì²­ ë¼ìš°íŒ… ---")
    llm = get_llm()
    route_prompt = ChatPromptTemplate.from_template(
        """ì‚¬ìš©ì ì§ˆë¬¸ '{question}'ì€ ë‹¤ìŒ ì¤‘ ì–´ë–¤ ìœ í˜•ì— ê°€ì¥ ê°€ê¹ìŠµë‹ˆê¹Œ?
        - ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•œ ì§ˆë¬¸: 'qa'
        - ì²¨ë¶€ëœ ë¬¸ì„œ(ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤)ì— ëŒ€í•œ í”¼ë“œë°± ìš”ì²­: 'feedback'
        ë‹µë³€ì€ ë°˜ë“œì‹œ 'qa' ë˜ëŠ” 'feedback' ë‹¨ì–´ í•˜ë‚˜ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."""
    )
    routing_chain = route_prompt | llm | StrOutputParser()
    result = routing_chain.invoke({"question": state["question"]})
    
    cleaned_result = clean_llm_output(result).lower()
    print(f"âœ… LLM ë¶„ê¸° íŒë‹¨ ê²°ê³¼: {cleaned_result}")

    if "feedback" in cleaned_result:
        return {"input_type": "feedback"}
    elif "qa" in cleaned_result:
        return {"input_type": "qa"}
    else:
        # ê¸°ë³¸ê°’ìœ¼ë¡œ QA ì„¤ì • ë˜ëŠ” ì—ëŸ¬ ì²˜ë¦¬
        print("âš ï¸ ë¼ìš°íŒ… ì‹¤íŒ¨, ê¸°ë³¸ê°’ 'qa'ë¡œ ì„¤ì •")
        return {"input_type": "qa"}
    
    
# --- ë‰´ìŠ¤ Q&A ê²½ë¡œ ---
@traceable(run_type="chain", name="Simple_Chain")
def get_tavily_snippets(state: GraphState):
    """
    Tavilyë¥¼ ì‚¬ìš©í•´ ê¸°ì—…ëª… + ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ì˜ ìµœì‹  ì›¹ ìŠ¤ë‹ˆí«ì„ ê²€ìƒ‰í•˜ì—¬ ë°˜í™˜.
    """
    print("--- 3a. Taviliy search ---")
    try:
        question = state.get('question')
        company_name = state.get('company')

        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        if company_name and question:
            search_query = f"{company_name} ê´€ë ¨ {question}"
        else:
            raise ValueError("ê²€ìƒ‰í•  ì§ˆë¬¸ê³¼ ê¸°ì—…ëª…ì´ ëª¨ë‘ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ” Tavily ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")

        # Tavily ê²€ìƒ‰
        results = tavily_tool.invoke(search_query)

        snippets = []
        for item in results.get('results', []):
            # ê²€ìƒ‰ëœ snippetê³¼ ì¶œì²˜ URL í•¨ê»˜ êµ¬ì„±
            snippet = f"{item.get('content', '').strip()}\nì¶œì²˜: {item.get('url', '').strip()}"
            if snippet.strip():
                snippets.append(snippet)

        print(f"âœ… Tavily: {len(snippets)}ê°œ ìŠ¤ë‹ˆí« ê²€ìƒ‰ ì™„ë£Œ")
        state["tavily_snippets"] = snippets

        return state

    except Exception as e:
        print(f"âš ï¸ Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return state
    

@traceable(run_type="chain", name="Simple_Chain")
def generate_answer_node(state: GraphState):
    print("--- 4a. ë‹µë³€ ìƒì„± ---")
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ [ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©]ê³¼ [ì›¹ ê²€ìƒ‰ ìŠ¤ë‹ˆí«]ì„ ì°¸ê³ í•˜ì—¬ [ì§ˆë¬¸]ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        [ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©]: {context}
        [ê¸°ì—… ê´€ë ¨ ê²€ìƒ‰ ìŠ¤ë‹ˆí«]: {web_snippets}
        [ì§ˆë¬¸]: {question}"""
    )
    rag_chain = prompt | llm | StrOutputParser()
    
    # Tavily snippet ì¶”ê°€
    tavily_snippets = state.get('tavily_snippets', [])
    tavily_context = "\n\n".join(tavily_snippets) if tavily_snippets else "ê²€ìƒ‰ëœ ì›¹ ìŠ¤ë‹ˆí« ì—†ìŒ."


    if not state['relevant_chunks']:
        answer = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ IDë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
    else:
        answer = rag_chain.invoke({
            "context": "\n---\n".join(state['relevant_chunks']),
            "web_snippets": tavily_context,
            "question": state['question']
        })
    state['answer'] = clean_llm_output(answer)
    return state

@traceable(run_type="chain", name="Simple_Chain")
def grade_answer_node(state: GraphState):
    print("--- 5a. ë‹µë³€ ê²€ì¦ ---")
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template(
        """[ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©]ì„ ë³¼ ë•Œ, [ìƒì„±ëœ ë‹µë³€]ì´ [ì§ˆë¬¸]ì— ëŒ€í•´ ì‚¬ì‹¤ì— ê·¼ê±°í•˜ëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
        ê·¼ê±°í–ˆë‹¤ë©´ 'yes', ì•„ë‹ˆë©´ 'no'ë¼ê³ ë§Œ ë‹µí•´ì£¼ì„¸ìš”.
        [ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©]: {context}
        [ì§ˆë¬¸]: {question}
        [ìƒì„±ëœ ë‹µë³€]: {answer}"""
    )
    grading_chain = prompt | llm | StrOutputParser()
    grade = grading_chain.invoke({
        "context": "\n---\n".join(state['relevant_chunks']),
        "question": state['question'],
        "answer": state['answer']
    })
    
    if "yes" in grade.lower():
        state['is_grounded'] = True
        print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼")
    else:
        state['is_grounded'] = False
        print("âŒ ê²€ì¦ ê²°ê³¼: ì‹¤íŒ¨")
    return state


# --- ë¬¸ì„œ í”¼ë“œë°± ê²½ë¡œ ---
@traceable(run_type="chain", name="Simple_Chain")
def load_and_summarize_resume_node(state: GraphState):

    print("--- 3b. ì´ë ¥ì„œ ë¡œë“œ ë° ìš”ì•½ ---")

    file_path = state['file_path']

    if not file_path or not os.path.exists(file_path):
        print("ğŸ“‚ íŒŒì¼ ê²½ë¡œê°€ ì—†ìœ¼ë¯€ë¡œ ChromaDBì—ì„œ ê¸°ì¡´ ìš”ì•½ì„ ì¡°íšŒí•©ë‹ˆë‹¤.")
        chroma_client = get_chroma_client()
        collection = chroma_client.get_or_create_collection(name="user_resume_db")

        query_id = f"{state['session_id']}_{state['user_id']}"
        results = collection.get(ids=[query_id])
        
        if results and results.get('documents'):
            state['user_file_summary'] = results['documents'][0]
            print(f"âœ… ChromaDBì—ì„œ ì´ë ¥ì„œ ìš”ì•½ ë³µì› ì™„ë£Œ: {state['user_file_summary']}...")
            return state
        else:
            raise ValueError(
                f"íŒŒì¼ ê²½ë¡œê°€ ì—†ê³  ChromaDBì—ë„ ìš”ì•½ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. "
                f"ì„¸ì…˜ {state.get('session_id')}, ì‚¬ìš©ì {state.get('user_id')}. "
                "í”¼ë“œë°±ì„ ì›í•˜ëŠ” íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì„¸ìš”."
            )
    
    else:
        if file_path.lower().endswith(".pdf"):
            loader = PyPDFLoader(file_path)
            pages = loader.load_and_split()
        else: # txt
            loader = TextLoader(file_path, encoding='utf-8')
            pages = loader.load_and_split()
        
        full_text = " ".join([page.page_content for page in pages])
        full_text = clean_pdf_text(full_text)

        # (ë¹„ìš©/ì‹œê°„ ê°œì„ ) ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•œë²ˆì— ìš”ì•½í•˜ë„ë¡ ë³€ê²½
        llm = get_llm()
        prompt = ChatPromptTemplate.from_template(
            "ë‹¤ìŒ ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ì˜ í•µì‹¬ ì—­ëŸ‰ê³¼ í”„ë¡œì íŠ¸ ê²½í—˜ì„ 3~5 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.\n\n{text}")
        summarization_chain = prompt | llm | StrOutputParser()
        summary = summarization_chain.invoke({"text": full_text})
        
        state['user_file_summary'] = f"{clean_llm_output(summary)}"
        print(f"âœ… ì´ë ¥ì„œ ìš”ì•½ ì™„ë£Œ: {state['user_file_summary']}...")  # ìš”ì•½ì˜ ì¼ë¶€ë§Œ ì¶œë ¥
        print("âœ… ì´ë ¥ì„œ ìš”ì•½ ì™„ë£Œ")

        # ---- store uploaded file summary text to chromadb -----

        # ChromaDBì— summary_textë¥¼ ì €ì¥
        chroma_client = get_chroma_client()
        collection = chroma_client.get_or_create_collection(name="user_resume_db")

        metadata = {
            "session_id": state['session_id'],
            "user_id": state['user_id'],
            "type": "resume_summary"
        }

        id_value = f"{state['session_id']}_{state['user_id']}"

        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ë®ì–´ì”Œì›€)
        collection.delete(ids=[id_value])

        # ChromaDBì— summary_textë¥¼ ì €ì¥ (ë‚´ìš© ê¸°ë°˜ ê²€ìƒ‰ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ dummy ì„ë² ë”© ì‚¬ìš© ê°€ëŠ¥)
        collection.add(
            documents=[state['user_file_summary']],
            metadatas=[metadata],
            ids=[id_value]
        )
        print(f"âœ… ì´ë ¥ì„œ ìš”ì•½ ë‚´ìš©ì„ ChromaDBì— ì €ì¥ ì™„ë£Œ (id={state['session_id']}_{state['user_id']})")

    return state

@traceable(run_type="chain", name="Simple_Chain")
def generate_resume_feedback_node(state: GraphState) -> GraphState:
    """
    ì´ë ¥ì„œ  ìš”ì•½ëœ ë‚´ìš© + ì§ˆë¬¸ + ê¸°ì—… ë‰´ìŠ¤ ìš”ì•½ ê¸°ë°˜ìœ¼ë¡œ
    ë§ì¶¤í˜• í”¼ë“œë°±ì„ ìƒì„±í•˜ì—¬ state["feedback"] ì— ì €ì¥
    """
    print("--- 4b. ë§ì¶¤í˜• ì´ë ¥ì„œ í”¼ë“œë°± ìƒì„± ---")
    
    llm = get_llm()

    # --- í”„ë¡¬í”„íŠ¸ ---
    prompt_template = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ì „ë¬¸ ì´ë ¥ì„œ ë° í¬íŠ¸í´ë¦¬ì˜¤ í”¼ë“œë°± ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

        ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì…ë‹ˆë‹¤:
        \"\"\"{question}\"\"\"

        
        ë‹¤ìŒì€ ì´ë ¥ì„œì˜ í•­ëª©ë³„ ìš”ì•½ ë‚´ìš©ì…ë‹ˆë‹¤:
        \"\"\"{resume_summary}\"\"\"

        # ë‹¤ìŒì€ ì§€ì›í•˜ë ¤ëŠ” ê¸°ì—…ì˜ ìµœê·¼ ë‰´ìŠ¤ ìš”ì•½ì…ë‹ˆë‹¤:
        \"\"\"{context}\"\"\"
                                                       
        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ì¡°ê±´ì— ë§ê²Œ í•œêµ­ì–´ë¡œ êµ¬ì²´ì ìœ¼ë¡œ í”¼ë“œë°±ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”:

        ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ë‹µë³€ ë° ê´€ë ¨ ë§¥ë½ ì–¸ê¸‰
        ë‰´ìŠ¤ ìš”ì•½ ë‚´ìš©ì„ ë°˜ì˜í•´ ê¸°ì—… ìƒí™©ê³¼ ì—°ê³„í•œ ì¸ì‚¬ì´íŠ¸ê°€ ìˆìœ¼ë©´ ì–¸ê¸‰
        êµ¬ì²´ì ì´ê³  ì‹¤ì§ˆì ìœ¼ë¡œ ë©´ì ‘ê³¼ ì¤€ë¹„ì— ë„ì›€ì´ ë˜ëŠ” í˜•íƒœ
        ì ì ˆí•œ ì˜ˆì‹œ ë¬¸ì¥ì„ í¬í•¨í•˜ì—¬ ì„¤ëª…

        ë‹¤ë¥¸ ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ì‘ì„±í•˜ì§€ ë§ê³ , í”¼ë“œë°±ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    """)

    

    # --- ì²´ì¸ ìƒì„± ---
    feedback_chain = prompt_template | llm | StrOutputParser()

    feedback = feedback_chain.invoke({
        "context": "\n---\n".join(state['relevant_chunks']),
        "question": state['question'],
        "resume_summary": state['user_file_summary']
    })

    cleaned_feedback = clean_llm_output(feedback)

    state["answer"] = cleaned_feedback

    print("âœ… ë§ì¶¤í˜• ì´ë ¥ì„œ í”¼ë“œë°± ìƒì„± ì™„ë£Œ")
    return state


# ==============================================================================
# 4. ê·¸ë˜í”„ êµ¬ì„± ë° ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================

def create_workflow():
    """LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤."""
    graph = StateGraph(GraphState)

    # ë…¸ë“œ ë“±ë¡
    graph.add_node("retrieve_from_chroma", retrieve_from_chroma_node)
    graph.add_node("route_request", route_request_node)
    graph.add_node("get_tavily_snippets", get_tavily_snippets)
    graph.add_node("generate_answer", generate_answer_node)
    # graph.add_node("grade_answer", grade_answer_node)
    graph.add_node("load_and_summarize_resume", load_and_summarize_resume_node)
    graph.add_node("generate_resume_feedback", generate_resume_feedback_node)
    # ê·¸ë˜í”„ íë¦„ ì •ì˜
    graph.set_entry_point("retrieve_from_chroma")

    graph.add_edge("retrieve_from_chroma", "route_request")

    # ë¼ìš°íŒ… ì¡°ê±´ ì„¤ì •
    graph.add_conditional_edges(
        "route_request",
        lambda state: state["input_type"],
        {
            "qa": "get_tavily_snippets",
            "feedback": "load_and_summarize_resume",
        }
    )

    # Q&A ê²½ë¡œ
    graph.add_edge("get_tavily_snippets", "generate_answer")
    graph.add_edge("generate_answer", END)

    # í”¼ë“œë°± ê²½ë¡œ
    graph.add_edge("load_and_summarize_resume", "generate_resume_feedback")
    graph.add_edge("generate_resume_feedback", END)  # í”¼ë“œë°± ìƒì„± í›„ ì¢…ë£Œ

    # ì›Œí¬í”Œë¡œìš°ë¥¼ ì»´íŒŒì¼í•˜ì—¬ ë°˜í™˜
    return graph.compile()
    
# ì›Œí¬í”Œë¡œìš° ì•±ì„ í•œë²ˆë§Œ ìƒì„±
agent_app = create_workflow()
    
    
# ==============================================================================
# 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë¸”ë¡
# ==============================================================================

if __name__ == "__main__":
    # ì´ ë¸”ë¡ì€ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ í…ŒìŠ¤íŠ¸í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    # FastAPI ì„œë²„ì—ì„œëŠ” ì´ ë¸”ë¡ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    

    # --- ì‹œë‚˜ë¦¬ì˜¤ 1: ë‰´ìŠ¤ Q&A í…ŒìŠ¤íŠ¸ ---
    print("\n" + "="*50)
    print("ì‹œë‚˜ë¦¬ì˜¤ 1: ë‰´ìŠ¤ Q&A í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*50)
    
    # ê°€ì •: news_id=101ì— í•´ë‹¹í•˜ëŠ” ë‰´ìŠ¤ê°€ ì´ë¯¸ ChromaDBì— ì €ì¥ë˜ì–´ ìˆìŒ
    # (ì‚¬ì „ ì‘ì—…: ë³„ë„ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë‰´ìŠ¤ë¥¼ ChromaDBì— ì €ì¥í•´ì•¼ í•¨)
    
    qa_input = {
        "question": "SKì‰´ë”ìŠ¤ê°€ ì œë¡œíŠ¸ëŸ¬ìŠ¤íŠ¸ ëª¨ë¸ë¡œ ë­˜ í•˜ë ¤ëŠ” ê±´ê°€ìš”?",
        "news_id": 101, # Spring ì„œë²„ë¡œë¶€í„° ë°›ì€ ë‰´ìŠ¤ ID
        "file_path": None,
        "company": "SKì‰´ë”ìŠ¤",  # íšŒì‚¬ëª… (ì¶”í›„ Tavily ê²€ìƒ‰ì— ì‚¬ìš©)
        "chat_history": []
    }
    
    try:
        # stream()ì„ ì‚¬ìš©í•˜ë©´ ê° ë‹¨ê³„ì˜ ì¶œë ¥ì„ ë³¼ ìˆ˜ ìˆìŒ
        for output in agent_app.stream(qa_input, {"recursion_limit": 10}):
            node_name = list(output.keys())[0]
            node_output = output[node_name]
            print(f"--- ë…¸ë“œ '{node_name}' ì‹¤í–‰ ì™„ë£Œ ---")
        
        final_state = agent_app.invoke(qa_input)
        

        print("\n[ìµœì¢… ë‹µë³€]:", final_state.get('answer'))

    except Exception as e:
        print(f"\n[ì˜¤ë¥˜ ë°œìƒ]: {e}")


    # --- ì‹œë‚˜ë¦¬ì˜¤ 2: ì´ë ¥ì„œ í”¼ë“œë°± í…ŒìŠ¤íŠ¸ ---
    print("\n" + "="*50)
    print("ì‹œë‚˜ë¦¬ì˜¤ 2: ì´ë ¥ì„œ í”¼ë“œë°± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*50)
    
    # í…ŒìŠ¤íŠ¸ìš© ì´ë ¥ì„œ íŒŒì¼ ìƒì„±
    resume_file = "./file_data/ì´ë ¥ì„œ_ì´ì¤€ê¸°.pdf"
    
    feedback_input = {
        "question": "ì œ ì´ë ¥ì„œì—ì„œ ìê¸°ì†Œê°œì„œë§Œ í”¼ë“œë°± í•´ì£¼ì„¸ìš”.",
        "news_id": None,
        "file_path": resume_file,
        "company": "SKì‰´ë”ìŠ¤",
        "chat_history": []
    }

    try:
        final_state = agent_app.invoke(feedback_input)
        print("\n[ìµœì¢… ë‹µë³€]:", final_state.get('answer'))
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜ ë°œìƒ]: {e}")
        
    # ê·¸ë˜í”„ ì‹œê°í™”
    try:
        graph_image_path = "agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            f.write(agent_app.get_graph().draw_mermaid_png())
        print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        