import os
import re
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI 
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from .chat_langgraph import get_chroma_client, get_embeddings
from langchain_chroma import Chroma
import json
from langsmith import Client
from langsmith import traceable



# ========== í™˜ê²½ ì„¤ì • ==========
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# LangSmith API Key ì„¤ì •
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
LANGSMITH_PROJECT = os.getenv("LANGSMITH_PROJECT", "llm-service-already")
LANGSMITH_TRACING = os.getenv("LANGSMITH_TRACING", "true").lower() == "true"
LANGSMITH_ENDPOINT = os.getenv("LANGSMITH_ENDPOINT")
client = Client(api_key=LANGSMITH_API_KEY)

# ========== LLM ë° Prompt ==========
def get_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜"""
    return ChatOpenAI(
        api_key=OPENAI_API_KEY,
        base_url="https://api.groq.com/openai/v1",
        model="meta-llama/llama-4-scout-17b-16e-instruct",
        temperature=0.7
    )

prompt = PromptTemplate(
    input_variables=["title", "content"],
    template="""
    ë‹¹ì‹ ì€ ê²½ì œ ë° ì‚°ì—… ë¶„ì„ì— ëŠ¥í•œ ë¦¬ì„œì²˜ì…ë‹ˆë‹¤.

    ì•„ë˜ëŠ” í•œ ê¸°ì—…ì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤.
    ì´ë ¥ì„œë¥¼ ì¤€ë¹„ ì¤‘ì¸ ì·¨ì—… ì¤€ë¹„ìƒì´ í•´ë‹¹ ê¸°ì—…ì„ ë¶„ì„í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë„ë¡, 
    í•µì‹¬ ë‚´ìš©ì„ 3~4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

    ìš”ì•½ì€ ë‹¤ìŒ ê¸°ì¤€ì— ë§ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”:
    - ê³¼ì¥ ì—†ì´, ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ
    - íˆ¬ììê°€ ì•„ë‹Œ ì·¨ì¤€ìƒì˜ ì‹œì„ ì—ì„œ ê¸°ì—…ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ë˜ë„ë¡
    - ë‹µë³€ë§Œ ì¶œë ¥

    [ì œëª©]
    {title}

    [ë‚´ìš©]
    {content}
    """)     

output_parser = StrOutputParser()

# ========== ìœ í‹¸ í•¨ìˆ˜ ==========
def clean_llm_output(text: str) -> str:
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE)
    text = text.strip()
    text = re.sub(r"```(?:markdown)?\s*(.*?)```", r"\1", text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

@traceable(run_type="chain", name="Simple_Chain")
def summarize_news(news_json: dict) -> str:
    news_id = str(news_json["id"])

    # 1ï¸âƒ£ ChromaDBì—ì„œ news_idì— í•´ë‹¹í•˜ëŠ” ì›ë¬¸ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    try:
        chroma_client = get_chroma_client()
        embeddings = get_embeddings()
        vectorstore = Chroma(
            client=chroma_client,
            collection_name="news_vector_db",
            embedding_function=embeddings,
        )

        retriever = vectorstore.as_retriever(
            search_kwargs={
                "filter": {"news_id": news_id},
                "k": 20  # í•„ìš”í•œ ë§Œí¼ ì¡°ì • ê°€ëŠ¥
            }
        )

        # dummy question (LLM ìš”ì•½ìš©ì´ë¯€ë¡œ ì§ˆë¬¸ ì˜ë¯¸ ì—†ìŒ)
        dummy_question = "ì´ ë‰´ìŠ¤ì˜ ì „ì²´ ë‚´ìš©ì„ ì£¼ì„¸ìš”."
        documents = retriever.invoke(dummy_question)

        if not documents:
            raise ValueError(f"news_id '{news_id}' ì— í•´ë‹¹í•˜ëŠ” ë‰´ìŠ¤ ì›ë¬¸ì„ ChromaDBì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ê°€ì ¸ì˜¨ ì²­í¬ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        content_chunks = []
        for doc in documents:
            raw_content = doc.page_content.strip()
            if not raw_content:
                continue
            # JSON í˜•íƒœë¡œ ì €ì¥ëœ ê²½ìš° í’€ì–´ì£¼ê¸°
            if raw_content.startswith('{') and raw_content.endswith('}'):
                try:
                    parsed = json.loads(raw_content)
                    extracted_text = parsed.get("data", {}).get("contents", raw_content)
                except Exception:
                    extracted_text = raw_content
            else:
                extracted_text = raw_content

            content_chunks.append(extracted_text)

        combined_content = "\n\n".join(content_chunks)

    except Exception as e:
        print(f"ğŸ”¥ ChromaDB ê²€ìƒ‰ ë° ê²°í•© ì¤‘ ì˜¤ë¥˜: {e}")
        combined_content = "(ì›ë¬¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"

    # 2ï¸âƒ£ LLM ìš”ì•½ í˜¸ì¶œ
    llm = get_llm()
    chain = prompt | llm | output_parser

    inputs = {
        "title": news_json.get("title", ""),
        "content": combined_content
    }

    raw_output = chain.invoke(inputs)
    return clean_llm_output(raw_output)

