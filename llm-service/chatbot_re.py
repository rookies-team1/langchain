import os
import json
import warnings
from dotenv import load_dotenv
from pathlib import Path
from collections import defaultdict
from typing import TypedDict, Optional, List, Dict, Any
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain, RetrievalQA
from langgraph.graph import StateGraph, END
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.messages import BaseMessage
import re

# Load API key and suppress warnings
load_dotenv()
warnings.filterwarnings("ignore")

# LLM ÏÑ§Ï†ï
llm_split = OllamaLLM(model="qwen3:1.7b")
llm_feedback = OllamaLLM(model="qwen3:1.7b")

# --- ÏÉÅÌÉú Ï†ïÏùò ---
class GraphState(TypedDict):
    user_question: Optional[str]
    chat_history: List[BaseMessage]
    file_path: Optional[str]
    pages: Optional[List]
    classified: Optional[List]
    section_map: Optional[Dict]
    vectorstore: Optional[Any]
    company_analysis: Optional[str]
    news_summary: Optional[str]
    feedback: Optional[str]
    answer: Optional[str]

# --- ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï ---
route_prompt = ChatPromptTemplate.from_template("""
    Îã§ÏùåÏùÄ ÏÇ¨Ïö©ÏûêÏùò ÏûÖÎ†•ÏûÖÎãàÎã§:

    "{question}"

    Ïù¥ ÏûÖÎ†•ÏùÄ Ïñ¥Îñ§ ÏûëÏóÖÏùÑ ÏöîÏ≤≠ÌïòÍ≥† ÏûàÎÇòÏöî?

    - ÏùºÎ∞ò Ï†ïÎ≥¥ ÏßàÎ¨∏Ïù¥Î©¥ "qa"
    - Ï≤®Î∂Ä Î¨∏ÏÑúÎ•º Í∏∞Î∞òÏúºÎ°ú ÌîºÎìúÎ∞± ÏöîÏ≤≠Ïù¥Î©¥ "feedback"

    Î∞òÎìúÏãú ÏúÑ Îã®Ïñ¥ Ï§ë ÌïòÎÇòÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî.
""")

route_chain = LLMChain(llm=llm_split, prompt=route_prompt)

split_prompt = ChatPromptTemplate.from_template("""
ÏïÑÎûòÎäî Ïù¥Î†•ÏÑú ÎòêÎäî Ìè¨Ìä∏Ìè¥Î¶¨Ïò§Ïùò Ìïú ÌéòÏù¥ÏßÄÏûÖÎãàÎã§:

"{page_text}"

Ïù¥ ÌéòÏù¥ÏßÄÎäî Ïñ¥Îñ§ Ìï≠Î™©Ïóê Ìï¥ÎãπÌïòÎÇòÏöî?
(Ïòà: Í≤ΩÎ†•, ÌïôÎ†•, ÌîÑÎ°úÏ†ùÌä∏, Í∏∞Ïà†, ÏûêÍ∏∞ÏÜåÍ∞ú Îì±)

Ìïú Îã®Ïñ¥Î°ú Ìï≠Î™©ÏùÑ Î∂ÑÎ•òÌïòÍ≥†, Í∞ÑÎã®Ìïú Ïù¥Ïú†Î•º ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.
""")

chain_split = LLMChain(llm=llm_split, prompt=split_prompt)

# --- ÎÖ∏Îìú Ï†ïÏùò ---
def route_by_input_type(state: GraphState) -> GraphState:
    result = route_chain.run(question=state["user_question"]).strip().lower()
    print(f"ü™ê LLM ÌåêÎã® Í≤∞Í≥º: {result}")

    if "feedback" in result:
        state = load_resume_pdf(state)
        state = classify_by_page(state)
        state = make_section_map(state)
        state = vector_indexing(state)
        state = load_company_analysis(state)
        state = match_and_feedback(state)
    elif "qa" in result:
        state = answer_question(state)
    else:
        raise ValueError(f"ÏßÄÏõêÎêòÏßÄ ÏïäÎäî ÏùëÎãµ: {result}")

    return state

def load_resume_pdf(state: GraphState) -> GraphState:
    loader = PyPDFLoader(state["file_path"])
    pages = loader.load()
    print(f"‚úÖ {len(pages)} ÌéòÏù¥ÏßÄ Î°úÎìú ÏôÑÎ£å")
    return {**state, "pages": pages}

def classify_by_page(state: GraphState) -> GraphState:
    results = []
    for idx, page in enumerate(state["pages"]):
        res = chain_split.run(page_text=page.page_content.strip())
        results.append({
            "page": idx + 1,
            "category": res,
            "content": page.page_content.strip()
        })
    return {**state, "classified": results}

def make_section_map(state: GraphState) -> GraphState:
    section_map = defaultdict(list)
    for item in state["classified"]:
        section = item['category'].split(":")[0].strip()
        section_map[section].append(item['content'])
    return {**state, "section_map": section_map}

def vector_indexing(state: GraphState) -> GraphState:
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    texts, metadatas = [], []
    for section, contents in state["section_map"].items():
        for content in contents:
            texts.append(content)
            metadatas.append({"section": section})
    vectorstore = FAISS.from_texts(texts, embeddings, metadatas)
    return {**state, "vectorstore": vectorstore}

def load_company_analysis(state: GraphState) -> GraphState:
    return state

def match_and_feedback(state: GraphState) -> GraphState:
    retriever = state["vectorstore"].as_retriever()
    qa_chain = RetrievalQA.from_chain_type(llm=llm_feedback, retriever=retriever)
    prompt = f"""
    Îã§ÏùåÏùÄ Ìïú Í∏∞ÏóÖÏùò Îâ¥Ïä§ Í∏∞ÏÇ¨ Î∂ÑÏÑù ÎÇ¥Ïö©ÏûÖÎãàÎã§:

    "{state['company_analysis']}"

    Ïù¥Î†•ÏÑú/Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Ìï≠Î™©Î≥ÑÎ°ú Í∞ïÏ°∞Ìï† Ï†ê, Î∂ÄÏ°±Ìïú Ï†ê, Î≥¥ÏôÑÏ†êÏùÑ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
    """
    raw_feedback = qa_chain.run(prompt)
    feedback = clean_llm_output(raw_feedback)
    print("‚úÖ ÌîºÎìúÎ∞± ÏÉùÏÑ± ÏôÑÎ£å")
    return {**state, "feedback": feedback}

def answer_question(state: GraphState) -> GraphState:
    if state.get("vectorstore"):
        retriever = state["vectorstore"].as_retriever()
        qa_chain = RetrievalQA.from_chain_type(llm=llm_feedback, retriever=retriever)
        raw_result = qa_chain.run(state["user_question"])
    else:
        raw_result = llm_feedback.invoke(state["user_question"])

    result = clean_llm_output(raw_result)
    print("‚úÖ ÏßàÎ¨∏ ÎãµÎ≥Ä ÏÉùÏÑ± ÏôÑÎ£å")
    return {**state, "answer": result}

def clean_llm_output(text: str) -> str:
    # <think>...</think> Î∏îÎ°ù Ï†úÍ±∞
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE)
    # Ï∂úÎ†• ÏãúÏûë/ÎÅùÏóê markdown blockÏù¥ ÎÇ®Îäî Í≤ΩÏö∞ Ï†úÍ±∞
    text = text.strip()
    # markdown block ÏïàÏóêÎßå ÎÇ®ÏïÑÏûàÎäî Í≤ΩÏö∞ ÏûòÎùºÎÇ¥Í∏∞
    # Ïòà: ```markdown ... ``` Íµ¨Ï°∞ Ï†úÍ±∞
    text = re.sub(r"```(?:markdown)?\s*(.*?)```", r"\1", text, flags=re.DOTALL | re.IGNORECASE)
    # Ïó∞ÏÜçÎêòÎäî 3Ï§Ñ Ïù¥ÏÉÅ Ï§ÑÎ∞îÍøàÏùÄ 2Ï§ÑÎ°ú Ï∂ïÏÜå
    text = re.sub(r"\n{3,}", "\n\n", text)
    # ÌïÑÏöî ÏóÜÎäî ÏÑ†Îëê/ÌõÑÎØ∏ Í≥µÎ∞± Ï†úÍ±∞
    return text.strip()


# --- Ïã§Ìñâ Ìï®Ïàò ---
def run_langgraph_flow(user_question: str,
                       resume_path: Optional[str] = None,
                       news_full_path: Optional[str] = None,
                       news_summary_path: Optional[str] = None,
                       chat_history: Optional[List[BaseMessage]] = None) -> Dict:
    state: GraphState = {"user_question": user_question, "chat_history": chat_history or []}

    if resume_path and Path(resume_path).exists():
        state["file_path"] = resume_path

    if news_summary_path and Path(news_summary_path).exists():
        with open(news_summary_path, "r", encoding="utf-8") as f:
            news_json = json.load(f)
            state["company_analysis"] = news_json.get("summary") or news_json.get("content")

    if news_full_path and Path(news_full_path).exists():
        with open(news_full_path, "r", encoding="utf-8") as f:
            full_news_text = f.read()
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
        vectorstore = FAISS.from_texts([full_news_text], embeddings)
        state["vectorstore"] = vectorstore

    graph = StateGraph(GraphState)

    graph.add_node("router", route_by_input_type)
    graph.add_node("ClassifyPages", classify_by_page)
    graph.add_node("ToSectionMap", make_section_map)
    graph.add_node("VectorIndexing", vector_indexing)
    graph.add_node("LoadCompanyInfo", load_company_analysis)
    graph.add_node("Feedback", match_and_feedback)

    graph.set_entry_point("router")

    graph.add_edge("ClassifyPages", "ToSectionMap")
    graph.add_edge("ToSectionMap", "VectorIndexing")
    graph.add_edge("VectorIndexing", "LoadCompanyInfo")
    graph.add_edge("LoadCompanyInfo", "Feedback")
    graph.add_edge("Feedback", END)

    compiled = graph.compile()
    result = compiled.invoke(state)
    print("‚úÖ ÌîåÎ°úÏö∞ Ïã§Ìñâ ÏôÑÎ£å")
    return result