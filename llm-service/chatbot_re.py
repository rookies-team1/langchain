import os
import json
import warnings
from dotenv import load_dotenv
from pathlib import Path
from collections import defaultdict
from typing import TypedDict, Optional, List, Dict, Any
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain, RetrievalQA
from langgraph.graph import StateGraph, END
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.messages import BaseMessage
import re


'''
================================================

ì‹¤í–‰ ë°©ë²• (llm-service ë””ë ‰í† ë¦¬ì—ì„œ):
uvicorn main:app --host 0.0.0.0 --port 8000 --reload

================================================
'''

# Load API key and suppress warnings
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
warnings.filterwarnings("ignore")

# LLM ì„¤ì •
# llm_split = OllamaLLM(model="qwen3:1.7b")
llm_split = ChatOpenAI(
    api_key=OPENAI_API_KEY,
    base_url="https://api.groq.com/openai/v1",  # Groq API ì—”ë“œí¬ì¸íŠ¸
    model="meta-llama/llama-4-scout-17b-16e-instruct",
    temperature=0.7
)
llm_feedback = OllamaLLM(model="qwen3:1.7b")

# --- ìƒíƒœ ì •ì˜ ---
class GraphState(TypedDict):
    user_question: Optional[str] # ì‚¬ìš©ì ì§ˆë¬¸
    chat_history: List[BaseMessage] # ëŒ€í™” ê¸°ë¡
    file_path: Optional[str] # ì²¨ë¶€íŒŒì¼ ê²½ë¡œ
    pages: Optional[List] # ì²¨ë¶€íŒŒì¼ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸
    classified: Optional[List] # í˜ì´ì§€ ë¶„ë¥˜ ê²°ê³¼
    section_map: Optional[Dict] # ì„¹ì…˜ë³„ ë‚´ìš© ë§µí•‘
    vectorstore: Optional[Any]  # ë²¡í„° ìŠ¤í† ì–´ (FAISS)
    company_analysis: Optional[str] # ê¸°ì—… ë¶„ì„ ë‚´ìš©
    news_summary: Optional[str] # ë‰´ìŠ¤ ìš”ì•½ ë‚´ìš©
    feedback: Optional[str] # í”¼ë“œë°± ë‚´ìš©
    answer: Optional[str] # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë‚´ìš©

def clean_llm_output(text: str) -> str:
    # LLMì˜ ì¶œë ¥ë¬¼ì—ì„œ í•„ìš” ì—†ëŠ” íƒœê·¸/ë§ˆí¬ë‹¤ìš´/ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì •ë¦¬

    # <think>...</think> ë¸”ë¡ ì œê±°
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE)
    # ì¶œë ¥ ì‹œì‘/ëì— markdown blockì´ ë‚¨ëŠ” ê²½ìš° ì œê±°
    text = text.strip()
    # markdown block ì•ˆì—ë§Œ ë‚¨ì•„ìˆëŠ” ê²½ìš° ì˜ë¼ë‚´ê¸°
    # ì˜ˆ: ```markdown ... ``` êµ¬ì¡° ì œê±°
    text = re.sub(r"```(?:markdown)?\s*(.*?)```", r"\1", text, flags=re.DOTALL | re.IGNORECASE)
    # ì—°ì†ë˜ëŠ” 3ì¤„ ì´ìƒ ì¤„ë°”ê¿ˆì€ 2ì¤„ë¡œ ì¶•ì†Œ
    text = re.sub(r"\n{3,}", "\n\n", text)
    # í•„ìš” ì—†ëŠ” ì„ ë‘/í›„ë¯¸ ê³µë°± ì œê±°
    return text.strip()

# --- í”„ë¡¬í”„íŠ¸ ì„¤ì • ---
route_prompt = ChatPromptTemplate.from_template("""
    ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì…ë ¥ì…ë‹ˆë‹¤:

    "{question}"

    ì´ ì…ë ¥ì€ ì–´ë–¤ ì‘ì—…ì„ ìš”ì²­í•˜ê³  ìˆë‚˜ìš”?

    - ì¼ë°˜ ì •ë³´ ì§ˆë¬¸ì´ë©´ "qa"
    - ì²¨ë¶€ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”¼ë“œë°± ìš”ì²­ì´ë©´ "feedback"

    ë°˜ë“œì‹œ ìœ„ ë‹¨ì–´ qaì™€ feedback ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    ì ˆëŒ€ ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
""")

# LLMChainìœ¼ë¡œ ë¼ìš°íŒ… ì²´ì¸ ìƒì„±
route_chain = LLMChain(llm=llm_split, prompt=route_prompt)

split_prompt = ChatPromptTemplate.from_template("""
    ì•„ë˜ëŠ” ì´ë ¥ì„œì˜ í•œ í˜ì´ì§€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

    =======================
    {page_text}
    =======================

    í…ìŠ¤íŠ¸ë¥¼ ì•„ë˜ í•­ëª©ë“¤ë¡œ ë‚´ìš©ì„ ë‚˜ëˆ„ì–´ ê°ê° í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì˜ ì›ë¬¸ ë‚´ìš©ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”:

    - ì¸ì ì‚¬í•­
    - í•™ë ¥
    - ê²½ë ¥
    - í”„ë¡œì íŠ¸
    - ê¸°ìˆ  ìŠ¤íƒ
    - ìˆ˜ìƒ ë° ìê²©ì¦
    - ìê¸°ì†Œê°œ

    ë‹¤ìŒ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

    [
        {{
            "category": "í•™ë ¥",
            "content": "í•œë°­ëŒ€í•™êµ ì „ìì „ê¸°ê³µí•™ê³¼ ..."
        }},
        {{
            "category": "ê²½ë ¥",
            "content": "í•œí™”ì‹œìŠ¤í…œ ì„¼í„° í•˜ê³„ ì¸í„´ì‹­ ..."
        }}
    ]

    ëŒ€ì‘ë˜ëŠ” í•­ëª©ì´ ì—†ìœ¼ë©´ Unknown í•­ëª©ìœ¼ë¡œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.
    ë°˜ë“œì‹œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
""")



# LLMChainìœ¼ë¡œ í˜ì´ì§€ ë¶„ë¥˜ ì²´ì¸ ìƒì„±
chain_split = LLMChain(llm=llm_split, prompt=split_prompt)

# --- ë…¸ë“œ ì •ì˜ ---
def route_by_input_type(state: GraphState) -> str:
    raw_result = route_chain.run(question=state["user_question"])
    result = clean_llm_output(raw_result).strip().lower()
    print(f"ğŸª LLM íŒë‹¨ ê²°ê³¼ : {result} ({type(result)})")

    if "feedback" in result:
        return "feedback"
    elif "qa" in result:
        return "qa"
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‘ë‹µ: {result}")


def router_node(state: GraphState) -> GraphState:
    # Entry point ì—­í• , ìƒíƒœ ê·¸ëŒ€ë¡œ ì „ë‹¬ë§Œ í•˜ë©´ ë¨
    return state

def load_resume_pdf(state: GraphState) -> GraphState:
    # PDF íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í˜ì´ì§€ë³„ ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    loader = PyPDFLoader(state["file_path"])
    pages = loader.load()
    print(f"âœ… {len(pages)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
    return {**state, "pages": pages}

def classify_by_page(state: GraphState) -> GraphState:
    # ê° í˜ì´ì§€ë¥¼ LLMì„ ì‚¬ìš©í•´ í•­ëª©ë³„ë¡œ ë¶„ë¥˜
    results = []
    for idx, page in enumerate(state["pages"]):
    #     print(f"--- í˜ì´ì§€ {idx + 1} ë¶„ë¥˜ ì¤‘ ---")
    #     print(f"í˜ì´ì§€ ë‚´ìš©:\n{page.page_content.strip()[:1000]}...")  # ì²˜ìŒ 1000ìë§Œ ì¶œë ¥
    #     raw_res = chain_split.invoke({"page_text": page.page_content.strip()})
    #     print(f"ğŸª LLM ì¶œë ¥ ê²°ê³¼: {raw_res}")

    #     try:
    #         parsed_res = json.loads(raw_res)
    #         for item in parsed_res:
    #             results.append({
    #                 "page": idx + 1,
    #                 "category": item["category"],
    #                 "content": item["content"].strip()
    #             })
    #     except Exception as e:
    #         # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜ì´ì§€ ì „ì²´ë¥¼ unknownìœ¼ë¡œ ì €ì¥í•´ ì¶”í›„ í™•ì¸ ê°€ëŠ¥
    #         print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    #         print(f"âš ï¸ LLM ì›ë³¸ ì¶œë ¥:\n{raw_res}")
    #         results.append({
    #             "page": idx + 1,
    #             "category": "Unknown",
    #             "content": page.page_content.strip()
    #         })

        paragraphs = [p.strip() for p in page.page_content.strip().split("\n\n+") if p.strip()]

        for p_idx, paragraph in enumerate(paragraphs):
            print(f"--- í˜ì´ì§€ {idx + 1}, ë¸”ë¡ {p_idx + 1} ë¶„ë¥˜ ì¤‘ ---")
            print(f"ë¸”ë¡ ë‚´ìš©: {paragraph[:300]}...")  # ê³¼ë„í•œ ì¶œë ¥ ë°©ì§€
            raw_res = chain_split.invoke({"page_text": paragraph})
            print(f"ğŸª LLM ì¶œë ¥ ê²°ê³¼: {raw_res}")

            try:
                parsed_res = json.loads(raw_res["text"] if isinstance(raw_res, dict) and "text" in raw_res else raw_res)
                for item in parsed_res:
                    results.append({
                        "page": idx + 1,
                        "category": item["category"],
                        "content": item["content"].strip()
                    })
            except Exception as e:
                print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                print(f"âš ï¸ LLM ì›ë³¸ ì¶œë ¥:\n{raw_res}")
                results.append({
                    "page": idx + 1,
                    "category": "Unknown",
                    "content": paragraph
                })


    output_path = "./file_data/classified_pages.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"âœ… ë¶„ë¥˜ ê²°ê³¼ê°€ '{output_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return {**state, "classified": results}


def make_section_map(state: GraphState) -> GraphState:
    # ë¶„ë¥˜ëœ í˜ì´ì§€ ë‚´ìš©ì„ ì„¹ì…˜ë³„ë¡œ ë¬¶ìŒ
    section_map = defaultdict(list)
    for item in state["classified"]:
        section = item["category"].split(":")[0].strip()
        section_map[section].append(item["content"])
    return {**state, "section_map": dict(section_map)}


def vector_indexing(state: GraphState) -> GraphState:
    # HuggingFace ì„ë² ë”© + FAISSë¥¼ ì‚¬ìš©í•´ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    texts, metadatas = [], []
    for section, contents in state["section_map"].items():
        for content in contents:
            texts.append(content)
            metadatas.append({"section": section})
    vectorstore = FAISS.from_texts(texts, embeddings, metadatas)
    return {**state, "vectorstore": vectorstore}

def load_company_analysis(state: GraphState) -> GraphState:
    # íšŒì‚¬ ë¶„ì„ ìš”ì•½ ë¡œë“œ (ì´ ë‹¨ê³„ì—ì„œ ì¶”ê°€ ì‘ì—…ì€ ì—†ì§€ë§Œ êµ¬ì¡°ìƒ í•„ìš”)
    return state

def match_and_feedback(state: GraphState) -> GraphState:
    # ë‰´ìŠ¤ ìš”ì•½ + ì´ë ¥ì„œ ë‚´ìš© + ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ LLMì´ í”¼ë“œë°± ìƒì„±
    retriever = state["vectorstore"].as_retriever()
    qa_chain = RetrievalQA.from_chain_type(llm=llm_feedback, retriever=retriever)

    prompt = f"""
    ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ì…ë‹ˆë‹¤:
    \"\"\"{state['user_question']}\"\"\"

    ì•„ë˜ëŠ” í•œ ê¸°ì—…ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤:
    \"\"\"{state['company_analysis']}\"\"\"

    ê·¸ë¦¬ê³  ì²¨ë¶€ëœ ì´ë ¥ì„œ ë˜ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

    ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ê³¼ ì²¨ë¶€ íŒŒì¼ì˜ ë‚´ìš©ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬,  
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ í•­ëª©ë³„ë¡œ ê°•ì¡°í•  ì , ë¶€ì¡±í•œ ì , ë³´ì™„í•  ì ì„ êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    """
    raw_feedback = qa_chain.run(prompt)
    feedback = clean_llm_output(raw_feedback)
    print("âœ… í”¼ë“œë°± ìƒì„± ì™„ë£Œ")
    return {**state, "feedback": feedback}

def answer_question(state: GraphState) -> GraphState:
    # ì¼ë°˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
    # ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ Retrieval QAë¡œ, ì—†ìœ¼ë©´ LLMìœ¼ë¡œ ì§ì ‘ ì‘ë‹µ ìƒì„±
    if state.get("vectorstore"):
        retriever = state["vectorstore"].as_retriever()
        qa_chain = RetrievalQA.from_chain_type(llm=llm_feedback, retriever=retriever)
        raw_result = qa_chain.run(state["user_question"])
    else:
        raw_result = llm_feedback.invoke(state["user_question"])

    result = clean_llm_output(raw_result)
    print("âœ… ì§ˆë¬¸ ë‹µë³€ ìƒì„± ì™„ë£Œ")
    return {**state, "answer": result}


# --- ì‹¤í–‰ í•¨ìˆ˜ ---
def run_langgraph_flow(user_question: str,
                       resume_path: Optional[str] = None,
                       news_full_path: Optional[str] = None,
                       news_summary_path: Optional[str] = None,
                       chat_history: Optional[List[BaseMessage]] = None) -> Dict:
    
    # ì´ˆê¸° ìƒíƒœ ì„¸íŒ…
    state: GraphState = {"user_question": user_question, "chat_history": chat_history or []}

    # ì´ë ¥ì„œ ê²½ë¡œê°€ ìˆìœ¼ë©´ ë¡œë“œ
    if resume_path and Path(resume_path).exists():
        state["file_path"] = resume_path

    # ë‰´ìŠ¤ ìš”ì•½ json ê²½ë¡œê°€ ìˆìœ¼ë©´ ë¡œë“œ
    if news_summary_path and Path(news_summary_path).exists():
        with open(news_summary_path, "r", encoding="utf-8") as f:
            news_json = json.load(f)
            state["company_analysis"] = news_json.get("summary") or news_json.get("content")

    # ë‰´ìŠ¤ ì „ë¬¸ ê²½ë¡œê°€ ìˆìœ¼ë©´ ë¡œë“œí•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    if news_full_path and Path(news_full_path).exists():
        with open(news_full_path, "r", encoding="utf-8") as f:
            full_news_text = f.read()
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
        vectorstore = FAISS.from_texts([full_news_text], embeddings)
        state["vectorstore"] = vectorstore

    # LangGraph ê°ì²´ ìƒì„±
    graph = StateGraph(GraphState)
    
    # ë…¸ë“œ ë“±ë¡
    # graph.add_node("router", route_by_input_type)
    graph.add_node("router", router_node)
    graph.add_node("LoadPDF", load_resume_pdf)
    graph.add_node("ClassifyPages", classify_by_page)
    graph.add_node("ToSectionMap", make_section_map)
    graph.add_node("VectorIndexing", vector_indexing)
    graph.add_node("LoadCompanyInfo", load_company_analysis)
    graph.add_node("Feedback", match_and_feedback)
    graph.add_node("AnswerQuestion", answer_question)

    graph.set_entry_point("router")

    # ë¼ìš°íŒ… ì¡°ê±´ ì„¤ì •: qa or feedback
    graph.add_conditional_edges("router", route_by_input_type, {
        "feedback": "LoadPDF",
        "qa": "AnswerQuestion"
    })

    # í”¼ë“œë°± í”Œë¡œìš° ì—°ê²°
    graph.add_edge("LoadPDF", "ClassifyPages")
    graph.add_edge("ClassifyPages", "ToSectionMap")
    graph.add_edge("ToSectionMap", "VectorIndexing")
    graph.add_edge("VectorIndexing", "LoadCompanyInfo")
    graph.add_edge("LoadCompanyInfo", "Feedback")
    graph.add_edge("Feedback", END)
    
    # QA í”Œë¡œìš° ì—°ê²°
    graph.add_edge("AnswerQuestion", END)

    # ê·¸ë˜í”„ ì‹œê°í™”
    # app = graph.compile()

    # try:
    #     graph_image_path = "langgraph_structure_nh.png"
    #     with open(graph_image_path, "wb") as f:
    #         f.write(app.get_graph().draw_mermaid_png())
    #     print(f"LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # except Exception as e:
    #     print(f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ê·¸ë˜í”„ ì»´íŒŒì¼ ë° ì‹¤í–‰
    compiled = graph.compile()
    result = compiled.invoke(state)
    print("âœ… í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ")
    return result

