services:
  ollama:
    image: ollama/ollama
    container_name: ollama_svc
    entrypoint: |
      /bin/bash -c "
      ollama serve &
      sleep 5 &&
      echo 'Pulling bge-m3:567m model...' &&
      ollama pull bge-m3:567m &&
      echo 'Model ready. Restarting server in foreground.' &&
      pkill ollama &&
      ollama serve
      "
    volumes:
      - ollama_data:/root/.ollama

  chroma:
    image: chromadb/chroma
    container_name: chroma_svc
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma

  llm-service:
    build: .
    image: kwonsoonmin/llm-service:dev02
    container_name: llm-svc
    depends_on:
      - ollama
      - chroma
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - CHROMA_HOST=chroma

volumes:
  ollama_data:
  chroma_data: