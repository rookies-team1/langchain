services:
  ollama:
    build:
      context: .
      dockerfile: ollama.Dockerfile
    container_name: ollama_svc
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  chroma:
    image: chromadb/chroma
    container_name: chroma_svc
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma

  llm-service:
    image: kwonsoonmin/llm-service:0.5
    container_name: llm-svc
    depends_on:
      - ollama
      - chroma
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - SPRING_SERVER_URL=${SPRING_SERVER_URL}
      - VECTOR_DB_HOST=${VECTOR_DB_HOST}
      - VECTOR_DB_PORT=${VECTOR_DB_PORT}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LANGSMITH_TRACING=true
      - LANGSMITH_PROJECT=llm-service-already
      - LANGSMITH_ENDPOINT=${LANGSMITH_ENDPOINT}
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY}
    entrypoint: ["/app/entrypoint.sh"]

networks:
  default:
    name: shared-network
    external: true

volumes:
  ollama_data:
  chroma_data:
